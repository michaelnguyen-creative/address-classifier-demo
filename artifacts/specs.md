## üéØ **What This Notebook Is Doing - A Conceptual Breakdown**

### **The Problem We're Solving**

Imagine you receive messy Vietnamese addresses from OCR (Optical Character Recognition) like:
- `"TT T√¢n B√¨nh Huy·ªán Y√™n S∆°n, Tuy√™n Quang"` 
- `"357/28,Ng-T- Thu·∫≠t,P1,Q3,TP.H·ªìCh√≠Minh."`

Your job: **Extract three pieces** ‚Üí `province`, `district`, `ward`

---

### **How the Notebook Works**

**Step 1: Setup & Data Loading**
```python
# The notebook provides 3 reference data files:
self.province_path = 'list_province.txt'  # 63 provinces
self.district_path = 'list_district.txt'  # Hundreds of districts  
self.ward_path = 'list_ward.txt'         # Thousands of wards
```

**Step 2: Your Task - Implement `process()`**
```python
def process(self, s: str):
    # Input: Messy address string
    # Output: Clean dictionary
    return {
        "province": "",  # You extract this
        "district": "",  # You extract this
        "ward": ""       # You extract this
    }
```

**Step 3: Testing & Scoring**
The notebook:
1. Downloads test data (450 test cases)
2. Runs your `process()` on each test
3. Handles **normalization** (e.g., "Ho√† B√¨nh" = "H√≤a B√¨nh")
4. Scores you: 1 point per correct field
5. Max score: 450 tests √ó 3 fields = **1,350 points**

**Step 4: Output**
Creates an Excel file with:
- Summary sheet (score, time)
- Details sheet (each test result)

---

### **The Algorithm Challenge**

Think of this as a **hierarchical search problem**:

```
Vietnam (63 provinces)
  ‚îú‚îÄ Province 1 (many districts)
  ‚îÇ   ‚îú‚îÄ District 1 (many wards)
  ‚îÇ   ‚îÇ   ‚îú‚îÄ Ward 1
  ‚îÇ   ‚îÇ   ‚îú‚îÄ Ward 2
  ‚îÇ   ‚îÇ   ‚îî‚îÄ ...
  ‚îÇ   ‚îî‚îÄ District 2
  ‚îî‚îÄ Province 2
```

**Key Algorithmic Concepts:**

1. **String Matching** (easy ‚Üí hard)
   - Exact match: `"H√† N·ªôi"` finds `"H√† N·ªôi"` ‚úì
   - Fuzzy match: `"Ha Noi"` finds `"H√† N·ªôi"` ‚úì 
   - Pattern match: `"TP.HCM"` finds `"H·ªì Ch√≠ Minh"` ‚úì

2. **Hierarchical Constraints**
   - Province ‚Üí District ‚Üí Ward flows **downward**
   - `"H√† ƒê√¥ng"` (district) ONLY exists under `"H√† N·ªôi"` (province)
   - Invalid: `"H√† ƒê√¥ng"` + `"Ngh·ªá An"` ‚úó

3. **Data Structures**
   - **Trie** for prefix matching
   - **Hash tables** for exact lookups  
   - **Graphs** for geographic relationships

---

### **What You Need to Build**

**Phase 1: Text Normalization** (Foundation)
```python
# Before: "Ng-T- Thu·∫≠t,P1,Q3,TP.H·ªìCh√≠Minh"
# After: "nguyen tu thuat p1 q3 tp ho chi minh"
```
- Remove accents (diacritics)
- Lowercase everything
- Handle abbreviations (`TP.` ‚Üí `thanh pho`)

**Phase 2: Pattern Recognition** (Core Algorithm)
```python
# Identify administrative levels:
patterns = {
    'province': ['t·ªânh', 'th√†nh ph·ªë', 'tp.', 't.'],
    'district': ['qu·∫≠n', 'huy·ªán', 'q.', 'h.'],  
    'ward': ['ph∆∞·ªùng', 'x√£', 'p.', 'x.']
}
```

**Phase 3: Matching Strategy** (Optimization)
```
Priority 1: Exact match (fast)
Priority 2: Pattern-based extraction (medium)
Priority 3: Fuzzy matching (slow, only if needed)
```

---

### **Performance Requirements**

| Metric | Target | Why It Matters |
|--------|--------|----------------|
| **Speed** | <0.1s per address | 450 tests in <45s |
| **Accuracy** | >85% (1,148/1,350) | Passing grade |
| **Memory** | Efficient data structures | No memory errors |

---

### **Key Insight for Algorithm Design**

**Think "Search Space Reduction":**

```
üî¥ BAD: Check ALL 63 provinces √ó ALL districts √ó ALL wards
        = Millions of comparisons per address

üü¢ GOOD: 
   1. Find province first (63 options)
   2. Only check districts in THAT province (20-50 options)
   3. Only check wards in THAT district (10-100 options)
        = ~100-200 comparisons per address
```

This is the **hierarchical constraint** in action!

